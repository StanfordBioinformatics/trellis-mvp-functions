import os
import re
import pdb
import sys
import json
import math
import time
import yaml
import base64
import neobolt

from datetime import datetime

#import neo4j
from neo4j import GraphDatabase

from urllib3.exceptions import ProtocolError
from neobolt.exceptions import ServiceUnavailable

from google.cloud import pubsub
from google.cloud import storage

import trellisdata as trellis

# Get runtime variables from cloud storage bucket
# https://www.sethvargo.com/secrets-in-serverless/
ENVIRONMENT = os.environ.get('ENVIRONMENT')
if ENVIRONMENT == 'google-cloud':

    # set up the Google Cloud Logging python client library
    # source: https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-python-client-library-v3-0-0-release
    import google.cloud.logging
    client = google.cloud.logging.Client()
    client.setup_logging()

    # use Python's standard logging library to send logs to GCP
    import logging

    FUNCTION_NAME = os.environ['FUNCTION_NAME']
    GCP_PROJECT = os.environ['GCP_PROJECT']

    config_doc = storage.Client() \
                .get_bucket(os.environ['CREDENTIALS_BUCKET']) \
                .get_blob(os.environ['CREDENTIALS_BLOB']) \
                .download_as_string()
    # https://stackoverflow.com/questions/6866600/how-to-parse-read-a-yaml-file-into-a-python-object
    TRELLIS = yaml.safe_load(config_doc)

    # Runtime variables
    #PROJECT_ID = parsed_vars['GOOGLE_CLOUD_PROJECT']
    #TOPIC_DB_QUERY = parsed_vars['TOPIC_DB_QUERY']
    #DB_STORED_QUERIES = parsed_vars['DB_STORED_QUERIES']

    #NEO4J_URL = parsed_vars['NEO4J_URL']
    #NEO4J_SCHEME = parsed_vars['NEO4J_SCHEME']
    #NEO4J_HOST = parsed_vars['NEO4J_HOST']
    #NEO4J_PORT = parsed_vars['NEO4J_PORT']
    #NEO4J_USER = parsed_vars['NEO4J_USER']
    #NEO4J_PASSPHRASE = parsed_vars['NEO4J_PASSPHRASE']

    # Pubsub client
    PUBLISHER = pubsub.PublisherClient()

    # Load queries predefined by Trellis developers.
    queries_document = storage.Client() \
                        .get_bucket(os.environ['CREDENTIALS_BUCKET']) \
                        .get_blob(TRELLIS["USER_DEFINED_QUERIES"]) \
                        .download_as_string()
    queries = yaml.load_all(queries_document, Loader=yaml.FullLoader)
    
    # Load list of existing queries that have been dynamically
    # generated by the create-blob-node function.
    create_blob_query_doc = storage.Client() \
                                .get_bucket(os.environ['CREDENTIALS_BUCKET']) \
                                .get_blob(TRELLIS["CREATE_BLOB_QUERIES"]) \
                                .download_as_string()
    create_blob_queries = yaml.load_all(create_blob_query_doc, Loader=yaml.FullLoader)

    # Load list of existing queries that have been dynamically
    # generated by the create-blob-node function.
    create_job_query_doc = storage.Client() \
                            .get_bucket(os.environ['CREDENTIALS_BUCKET']) \
                            .get_blob(TRELLIS["CREATE_JOB_QUERIES"]) \
                            .download_as_string()
    create_job_queries = yaml.load_all(create_job_query_doc, Loader=yaml.FullLoader)

    QUERY_DICT = {}
    for query in queries:
        if query.name in QUERY_DICT.keys():
            raise ValueError(f"Query {query.name} is defined in duplicate.")
        QUERY_DICT[query.name] = query

    for query in create_blob_queries:
        if query.name in QUERY_DICT.keys():
            raise ValueError(f"Query {query.name} is defined in duplicate.")
        QUERY_DICT[query.name] = query

    for query in create_job_queries:
        if query.name in QUERY_DICT.keys():
            raise ValueError(f"Query {query.name} is defined in duplicate.")
        QUERY_DICT[query.name] = query

    # Use Neo4j driver object to establish connections to the Neo4j
    # database and manage connection pool used by neo4j.Session objects
    # https://neo4j.com/docs/api/python-driver/current/api.html#driver
    DRIVER = GraphDatabase.driver(
        f"{TRELLIS['NEO4J_SCHEME']}://{TRELLIS['NEO4J_HOST']}:{TRELLIS['NEO4J_PORT']}",
        auth=("neo4j", TRELLIS["NEO4J_PASSPHRASE"]),
        max_connection_pool_size=10)
else:
    FUNCTION_NAME = 'db-query-local'
    local_queries = "sample-queries.yaml"

    # Load database queries into a dictionary
    with open(local_queries, "r") as file_handle:
        queries = yaml.load_all(file_handle, Loader=yaml.FullLoader)
        QUERY_DICT = {}
        for query in queries:
            QUERY_DICT[query.name] = query

QUERY_ELAPSED_MAX = 300
PUBSUB_ELAPSED_MAX = 10

#def query_database(write_transaction, driver, query, query_parameters):
def query_database(driver, query, parameters):
    """Run a Cypher query against the Neo4j database.

    Args:
        write_transaction (bool): Indicate whether write permission is needed.
        driver (neo4j.Driver): Official Neo4j Python driver
        query (str): Parameterized Cypher query
        query_parameters (dict): Parameters values that will be used in the query.
    Returns:
        neo4j.graph.Graph: https://neo4j.com/docs/api/python-driver/current/api.html#neo4j.graph.Graph
        neo4j.ResultSummary: https://neo4j.com/docs/api/python-driver/current/api.html#resultsummary
    """

    # Check whether query parameters match the required keys and types
    key_difference = set(query.required_parameters.keys()).difference(set(parameters.keys()))
    if key_difference:
        #logging.error(f"Query parameters do not match requirements. Difference: {key_difference}.")
        raise ValueError(f"Query parameters do not match requirements. Difference: {key_difference}.")

    for key, type_name in query.required_parameters.items():
        if not type(parameters[key]).__name__ == type_name:
            raise ValueError(f"Query parameter {parameters[key]} does not match type {type_name}.")

    with driver.session() as session:
        if query.write_transaction:
            graph, result_summary = session.write_transaction(_stored_procedure_transaction_function, query.cypher, **parameters)
        else:
            graph, result_summary = session.read_transaction(_stored_procedure_transaction_function, query.cypher, **parameters)
    return graph, result_summary

def _stored_procedure_transaction_function(tx, query, **query_parameters):
    result = tx.run(query, query_parameters)
    # Return graph data and ResultSummary object
    return result.graph(), result.consume()

def main(event, context, local_driver=None):
    """When an object node is added to the database, launch any
       jobs corresponding to that node label.

       Args:
            event (dict): Event payload.
            context (google.cloud.functions.Context): Metadata for the event.
    """

    start = datetime.now()
    query_request = trellis.QueryRequestReader(
                                               event=event, 
                                               context=context)
    
    print(f"> Received message context: {query_request.context}.")
    print(f"> Received message header: {query_request.header}.")
    print(f"> Received message body: {query_request.body}.")

    if ENVIRONMENT == 'google-cloud':
        # Time from message publication to reception
        request_publication_time = trellis.utils.convert_timestamp_to_rfc_3339(query_request.context.timestamp)
        publish_elapsed = datetime.now() - request_publication_time
        if publish_elapsed.total_seconds() > PUBSUB_ELAPSED_MAX:
            print(
                  f"> Time to receive message ({int(publish_elapsed.total_seconds())}) " +
                  f"exceeded {PUBSUB_ELAPSED_MAX} seconds after publication.")

    # TODO: maybe I should store custom queries after they've been
    # created and give them unique IDs based on content so I can 
    # lookup already used queries
    if query_request.custom == True:
        # If it's a custom query, let's use it regardless. The only question is whether
        # it's a new query, in which case we should add to a list of custom queries.

        # Instead of creating instance at the end of if/then logic block,
        # create it at the beginning so we can use the __eq__ function.
        # Regardless of whether or not it's novel, we are going to use this
        # instance.
        required_parameters = {}
        for key, value in query_request.query_parameters.items():
            required_parameters[key] = type(value).__name__

        database_query = trellis.DatabaseQuery(
            name=query_request.query_name,
            cypher=query_request.cypher,
            # Maybe I should populate this
            required_parameters=required_parameters,
            write_transaction=query_request.write_transaction,
            returns = query_request.returns,
            publish_to = query_request.publish_to,
            split_results = query_request.split_results,
            active = True)

        register_new_query = True
        
        # If query has already been stored, check that new version matches
        # stored version and use stored version.
        if database_query.name in QUERY_DICT.keys():
            if database_query == QUERY_DICT[database_query.name]:
                logging.info(f"Query {database_query.name} alread stored.")
                #register_new_query = False
            else:
                raise ValueError(f"Custom query {database_query.name} is already stored "
                                 f"but does not match new query: {database_query}.")
        # If query has NOT been stored and is a create-blob-node query
        elif re.match(database_query.name, "^mergeBlob*"):
            # Reload create blob queries to make sure list is current
            # Read existing create-blob-queries
            create_blob_query_doc = storage.Client() \
                                    .get_bucket(os.environ['CREDENTIALS_BUCKET']) \
                                    .get_blob(TRELLIS["CREATE_BLOB_QUERIES"]) \
                                    .download_as_string()
            create_blob_queries = yaml.load_all(create_blob_query_doc, Loader=yaml.FullLoader)
            # Now I've loaded ALL of the queries once
            # and I'm loading all of these queries again
            # Another way to do this would be to add the queries to a database
            # ANOTHER way to do this would just be use memory.
            # Or I can just purge the existing QUERY_DICT before loading these

            # I don't want to load these into the QUERY_DICT because that has all queries
            for query in create_blob_queries:
                if database_query.name == query.name:
                    if database_query == query:
                        logging.info(f"Query {database_query.name} alread stored.")
                        register_new_query = False
                    else:
                        raise ValueError(f"Custom query {database_query.name} is already stored in " +
                                         f"{TRELLIS['CREATE_BLOB_QUERIES']} " +
                                         f"but does not match new query: {database_query}.")
            if register_new_query:
                create_blob_query_str = "--- "
                create_blob_query_str += yaml.dump_all(create_blob_queries)
                create_blob_query_str += "--- "
                create_blob_query_str += yaml.dump(database_query)
            create_blob_query_doc = storage.Client() \
                                        .get_bucket(os.environ['CREDENTIALS_BUCKET']) \
                                        .get_blob(TRELLIS["CREATE_BLOB_QUERIES"]) \
                                        .upload_from_string(create_blob_query_str)
        # Register new create-job-node queries
        elif re.match(database_query.name, "^mergeJob*"):
            # Reload create job queries to make sure list is current
            create_job_query_doc = storage.Client() \
                                    .get_bucket(os.environ['CREDENTIALS_BUCKET']) \
                                    .get_blob(TRELLIS["CREATE_JOB_QUERIES"]) \
                                    .download_as_string()
            create_job_queries = yaml.load_all(create_job_query_doc, Loader=yaml.FullLoader)

            for query in create_job_queries:
                if database_query.name == query.name:
                    if database_query == query:
                        logging.info(f"Query {database_query.name} alread stored.")
                        register_new_query = False
                    else:
                        raise ValueError(f"Custom query {database_query.name} is already stored in " +
                                         f"{TRELLIS['CREATE_BLOB_QUERIES']} " +
                                         f"but does not match new query: {database_query}.")
            if register_new_query:
                create_job_query_str = "--- "
                create_job_query_str += yaml.dump_all(create_blob_queries)
                create_job_query_str += "--- "
                create_job_query_str += yaml.dump(database_query)
            create_job_query_doc = storage.Client() \
                                        .get_bucket(os.environ['CREDENTIALS_BUCKET']) \
                                        .get_blob(TRELLIS["CREATE_JOB_QUERIES"]) \
                                        .upload_from_string(create_job_query_str)
    else:
        database_query = QUERY_DICT[query_request.query_name]

    try:
        # TODO: Compare the provided query parameters against the 
        # required query parameters
        logging.info(f"> Running query: {database_query.name}.")
        graph, result_summary = query_database(
            driver = DRIVER,
            query = database_query,
            parameters = query_request.query_parameters)
    except ProtocolError as error:
        logging.error(f"> Encountered Protocol Error: {error}.")
        # Add message back to queue
        #result = republish_message(DB_QUERY_TOPIC, data)
        #logging.warn(f"> Published message to {DB_QUERY_TOPIC} with result: {result}.")
        # Duplicate message flagged as warning
        #logging.warn(f"> Encountered Protocol Error: {error}.")
        return
    except ServiceUnavailable as error:
        logging.error(f"> Encountered Service Interrupion: {error}.")
        # Remove this connection(?) - causes UnboundLocalError
        #GRAPH = None
        # Add message back to queue
        #result = republish_message(DB_QUERY_TOPIC, data)
        #logging.warn(f"> Published message to {DB_QUERY_TOPIC} with result: {result}.")
        # Duplicate message flagged as warning
        #logging.warn(f"> Requeued message: {pubsub_message}.")
        return
    except ConnectionResetError as error:
        logging.error(f"> Encountered connection interruption: {error}.")
        # Add message back to queue
        #result = republish_message(DB_QUERY_TOPIC, data)
        #logging.warn(f"> Published message to {DB_QUERY_TOPIC} with result: {result}.")
        # Duplicate message flagged as warning
        #logging.warn(f"> Requeued message: {pubsub_message}.")
        return

    result_available_after = result_summary.result_available_after
    result_consumed_after = result_summary.result_consumed_after
    logging.info(f"> Query result available after: {result_available_after} ms.")
    logging.info(f"> Query Result consumed after: {result_consumed_after} ms.")
        #print(f"> Elapsed time to run query: {query_elapsed:.3f}. Query: {query}.")
    if int(result_available_after) > QUERY_ELAPSED_MAX:
        logging.warning(f"> Result available time ({result_available_after} ms) " +
                        f"exceeded {QUERY_ELAPSED_MAX:.3f}. " +
                        f"Query: {database_query.name}.")
    logging.info(f"> Query result counter: {result_summary.counters}.")

    query_response = trellis.QueryResponseWriter(
        sender = FUNCTION_NAME,
        seed_id = query_request.seed_id,
        previous_event_id = query_request.event_id,
        query_name = query_request.query_name,
        graph = graph,
        result_summary = result_summary)

    # Return if no pubsub topic or not running on GCP
    if not database_query.publish_to or not ENVIRONMENT == 'google-cloud':
        print("No Pub/Sub topic specified; result not published.")
    else:
        # Track how many messages are published to each topic
        published_message_counts = {}
        for topic_name in database_query.publish_to:
            topic = TRELLIS[topic_name]
            # TODO: map the name in the query definition to the real topic

            published_message_counts[topic] = 0

            if database_query.split_results == 'True':
                """ I think we can always send a result because even if no
                    node or relationship is returned we may still want to
                    use the summary statistics.

                if not query_results:
                    # If no results; send one message so triggers can respond to null
                    query_result = {}
                    message = format_pubsub_message(
                                                    method = method,
                                                    labels = labels,
                                                    query = query,
                                                    results = query_result,
                                                    seed_id = seed_id,
                                                    event_id = event_id,
                                                    retry_count=retry_count)
                    print(f"> Pubsub message: {message}.")
                    publish_result = publish_to_topic(topic, message)
                    print(f"> Published message to {topic} with result: {publish_result}.")
                    published_message_counts[topic] += 1
                """
                for message in query_response.format_json_message_iter():
                    logging.info(f"> Publish topic: {topic}, message: {message}.")
                    publish_result = trellis.utils.publish_to_pubsub_topic(
                        publisher = PUBLISHER,
                        project_id = GCP_PROJECT,
                        topic = topic, 
                        message = message)
                    logging.info(f"> Published message to {topic} with result: {publish_result}.")
                    published_message_counts[topic] += 1
            else:
                message = query_response.format_json_message()
                logging.info(f"> Publish topic: {topic}, message: {message}.")
                publish_result = trellis.utils.publish_to_pubsub_topic(
                        publisher = PUBLISHER,
                        project_id = GCP_PROJECT,
                        topic = topic, 
                        message = message)
                logging.info(f"> Published message to {topic} with result: {publish_result}.")
                published_message_counts[topic] += 1
        logging.info(f"> Summary of published messages: {published_message_counts}")

    ## Execution time block
    #end = datetime.now()
    #execution_time = (end - start).seconds
    #time_threshold = int(execution_time/10) * 10
    #if time_threshold > 0:
    #    print(f"> Execution time exceeded {time_threshold} seconds.")
